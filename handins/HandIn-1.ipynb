{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining - Handin 1 - Clustering and outlier detection\n",
    "Welcome to the handin on clustering algorithms and outlier detection. \n",
    "This handin corresponds to the topics in Week 5--9 in the course.\n",
    "\n",
    "The handin IS \n",
    "* mandatory\n",
    "* done individually\n",
    "* worth 10% of the grade\n",
    "\n",
    "For the handin, you will prepare a report in PDF format, by exporting the Jupyter notebook. \n",
    "Please submit\n",
    "1. The jupyter notebook file with your answers\n",
    "2. The PDF obtained by exporting the jupyter notebook\n",
    "\n",
    "Submit both files on Blackboard no later than **March 14th kl. 23.59**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Theoretical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You are given a metric $d_{old}(x,y)$ where $x,y\\in\\mathbb{R}$, prove that $d_{new}(x,y)=\\frac{d_{old}(x,y)}{1+d_{old}(x,y)}$ is also a metric\n",
    "    - Suggestion: remember monotonicity of $f(t)=\\frac{t}{1+t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Show that $\\hat{\\Sigma}=\n",
    "\\frac{1}{n}\\sum_{i=1}^n (x_i -\\hat{\\mu}^\\top)\\cdot(x_i -\\hat{\\mu}^\\top)^\\top=E[(X-\\hat{\\mu})(X-\\hat{\\mu})^\\top]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **The calculations in this exercise have to be done by hand**. Given the dataset below:\n",
    "    - Calculate by hand the cluster assignments using k-means and $k=2$ and report the results\n",
    "    - Show two examples with two different initial cluster assignments that lead to a different result.\n",
    "    - Compute the dendrogram\n",
    "    - Run density-based clustering with $\\epsilon=1$ and $MinPts=2$\n",
    "\n",
    "![image.png](images/data_1_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. After looking at the dataset below, you realize that the clusters are elliptic rather than spherical. You want to detect the red outlier point, assuming you know that is an outlier. \n",
    "    * Would you use a distance-based outlier detector to detect the outlier? \n",
    "    * Would you use a depth-based approach? \n",
    "    * Motivate your answers! \n",
    "\n",
    "![image.png](images/outliers.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Consider the following scenario: you have been asked to provide a subspace clustering of a data set. Since you already have an implementation of PCA for dimensionality reduction and a full space clustering (such as k-means or DBSCAN, the precise choice is not relevant for this question) available, you consider just using PCA and then full space clustering. Are the results expected to be similar? Why/why not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preliminary analyses and considerations\n",
    "\n",
    "In this section, you will perform preliminary analyses on your data. These preliminary analysis are useful to understand how the data behaves, before running complex algorithms.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "import warnings\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import time\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_online(url, headers, delimiter=\",\"): \n",
    "    s = requests.get(url).content\n",
    "    data = pd.read_csv(io.StringIO(s.decode('utf-8')), sep=delimiter, names=headers)\n",
    "    return data\n",
    "\n",
    "headers = ['class', 'Alcohol', 'Malic_acid', 'Ash', 'Alcalinity_of_ash', 'Magnesium', 'Total_phenols', 'Flavanoids', 'Nonflavanoid_phenols', 'Proanthocyanins', 'Color_intensity', 'Hue', 'OD280_OD315', 'Proline']\n",
    "wines = read_data_online(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\", headers, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wines.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = wines.to_numpy()\n",
    "X = data[:,1:]\n",
    "y = data[:,0]\n",
    "y = y.astype(int) - 1\n",
    "rows, cols = np.shape(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compute the covariance matrix among all the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Normalize the features using standard score normalization and range normalization. Plot the two covariance matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss how the covariance matrix change and find a reason why such behaviour appears. What are the differences between the two normalizations? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Sometimes it is convenient to know whether a variable is close to a normal distribution. Implement a method norm_dist that: \n",
    "    1) Inputs the number of buckets $b$ and a vector $x$ of values\n",
    "    2) Outputs the sum of the absolute differences of the buckets between the a histogram with $b$ buckets of a normal variable with $\\mu,\\sigma$ deviation corresponding sample mean and standard deviation of the input vector and the histogram of the input vectors with $b$ buckets. The sum of the differences is computed as \n",
    "    \n",
    "    $$\\sum_{i=1}^b |H_X(i) - H_{\\mathcal{N}}(i)|$$ \n",
    "    \n",
    "    where $H_X(i)$ is the i-th bucket of the histogram of $x$ and $H_\\mathcal{N}(i)$ is the i-th bucket of the hisotgram obtained from the normal distribution $\\mathcal{N}(\\mu,\\sigma^2)$. \n",
    "\n",
    "    Is the method a good method? Is it robust to outliers? Run your code on each columns of the dataset. What is the one with the largest distance? Compare the norm_dist of each attribute feature in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_dist(x, b): \n",
    "    dist = 0\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at the method below. This is call a Quantile-Quantile [Q-Q plot](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from matplotlib import gridspec\n",
    "\n",
    "_, n = data.shape\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig = plt.figure(constrained_layout=True, figsize=(8, 30))\n",
    "spec = gridspec.GridSpec(ncols=2, nrows=(n-1), figure=fig)\n",
    "for i in np.arange(1,n): \n",
    "    x = data[:,i]\n",
    "    r = i-1\n",
    "    qq = fig.add_subplot(spec[r, 1]) \n",
    "    stats.probplot(x, plot=qq)\n",
    "    h = fig.add_subplot(spec[r, 0]) \n",
    "    h.set_title(headers[i])\n",
    "    h.hist(x, bins = 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Discuss why this method is more robust than the one we proposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Find the two attributes that are correlated the least among each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cluster analysis\n",
    "\n",
    "In this section, you will perform cluster analysis of the dataset above and modify clustering algorithms to achieve better results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement and use the elbow method detect the number of clusters. For plotting you can use distortion, which is the sum of squared distances to the assigned cluster centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Run k-means on the dataset, with the number of clusters detected in the previous exercise. \n",
    "\n",
    "**Note**: you can use the KMeans implementation from scikit-learn\n",
    "\n",
    "```python\n",
    "kmeans = sklearn.cluster.KMeans(n_clusters).fit(X)\n",
    "clusters = kmeans.labels_\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implement Kernel K-means and the Gaussian Kernel. \n",
    "\n",
    "The Gaussian kernel is defined as in the following equation:\n",
    "\n",
    "$$\n",
    "K\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right)=\\exp \\left(-\\frac{\\left\\|\\mathbf{x}_{i}-\\mathbf{x}_{j}\\right\\|^{2}}{2 \\sigma^{2}}\\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_kernel(x, y, sigma=0.8): \n",
    "    k = 0 \n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    return k\n",
    "\n",
    "\n",
    "def kernel_kmeans(X, n_clusters, kernel=gaussian_kernel, iters=100, error=.01): \n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    return clusters\n",
    "\n",
    "clusters = kernel_kmeans(X_norm, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now visualize our clusters with the method below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT MODIFY\n",
    "\n",
    "def plot_clusters(X, clusters): \n",
    "\n",
    "    time_start = time.time()\n",
    "    tsne = TSNE(n_components=2, verbose=1, random_state=0)\n",
    "    X_2d = tsne.fit_transform(X)\n",
    "    print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    colors = ['b','r','g', 'm', 'c', 'y', 'k', 'tan', 'gold','sienna', 'navy','darkgreen']\n",
    "    for i in np.arange(np.shape(data)[0]):\n",
    "        plt.scatter(X_2d[i, 0], X_2d[i, 1], c=colors[clusters[i]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes your normalized data has been saved as \"X_norm\"\n",
    "plot_clusters(X_norm, y)\n",
    "plot_clusters(X_norm, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization above, use a method called [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) to plot the data into a 2D space. \n",
    "The method is not part of the course, but it is a good tool for data analysis that should be always considered. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Above you have seen how to measure cluster quality. Now we will implement, [Normalized Mutual Information](https://en.wikipedia.org/wiki/Mutual_information), another measure for cluster quality. Mutual information for two clusterings $U$ and $V$ is defined as $\\text{MI}(U,V) = \\sum\\limits^{|U|}_{i=1} \\sum\\limits^{|V|}_{j=1} \\frac{|U_i \\cap V_j|}{N} \\log \\frac{|U_i \\cap V_j|}{|U_i||V_j|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMI(C1, C2):\n",
    "    mi = 0\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    return mi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Plot the NMI among the class labels $y$ and the clusters you found with k-means. \n",
    "    * Reason about the measure, why is it a good measure? \n",
    "    * What does the measure capture? \n",
    "    * Plot purity and compare the two measures at increasing number of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Inspect again the clustering results above, both in terms of NMI and with t-SNE. \n",
    "    * What other clustering algorithm could be used instead of K-Means? \n",
    "    * Would Density-based clustering work better? Why? \n",
    "    * Run DB-Scan with parameters $\\varepsilon=0.5, minPts=3$ from sklearn and compare the results with k-means. What do you notice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER\n",
    "\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=3).fit(X_norm)\n",
    "clusters_dbs = dbscan.labels_\n",
    "plot_clusters(X_norm, clusters_dbs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. DBScan requires tuning of parameters $\\varepsilon, MinPts$. Implement the heuristic strategy  in the slides to find the best parameters. Run the code and see whether the results with DBScan improve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_dbscan(X): \n",
    "    eps = 0\n",
    "    pi = 0\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    return eps, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. DBScan is an expensive algorithm. Assume you have been given e very big dataset. After thinking, you realise that you can create a number of samples of the points, run DBScan on the samples, assign the points to the closest cluster. \n",
    "    * What is the disadvantage of such an approach? \n",
    "    * What is wrong if you consider a uniform sample (every point equal probability) without replacement? What kind of sampling strategy would you devise to preserve the results of DBScan from the original dataset to the sampled one?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. In this last point, we will try to implement a simple subspace clustering algorithm and compare the results with CLIQUE imeplemented in Week 8. \n",
    "    * Take all subsets of 2,3 attributes. \n",
    "    * Run dbscan on each subset. \n",
    "    * Compute NMI for each subset. \n",
    "    * Keep the k subsets with the largest NMI. \n",
    "    \n",
    "**Note**: You may have to experiment a lot with eps and MinPts to get reasonable clusterings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze: \n",
    "* Advantages and disadvantages of the proposed algorithm\n",
    "* Results with respect to CLIQUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Outlier detection\n",
    "In this exercise we will work with outlier detection techniques and analyze their performance on the small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We will now compare two outlier detection techniques. We will first implement a simple distance-based outlier detector. This is the distance-based outlier detection from the lectures, where a point is considered an outlier if at most a fraction pi of the other points have a distance less of than eps to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DBOutliers(X, eps, pi): \n",
    "    outliers = None\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    return outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. DBOutliers requires tuning the parameters eps, pi. Discuss how the results vary with those parameters. Plot the number of outliers as a function of eps and as a function of pi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Propose a heuristic method to tune parameters eps, pi. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_dboutliers(X): \n",
    "    eps = 0\n",
    "    pi = 0\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    return eps, pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Using the parameters eps=0.9, pi=0.8 (and using the normalized data-set) compare the results of DBOutliers with those of LOF. What are the main differences? What outliers do you find? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. In the last exercise we will try to detect outliers in different subspaces in a way similar to exercise 3.9, but we in this case we will use the Local Outlier Factor (LOF) measure with $k=3$ to determine the quality of the clasters. \n",
    "\n",
    "Take the subspace with the highest LOF, what outliers do you obtain? Looking at the dataset and the LOF measure, can you explain why these are considered outliers? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
